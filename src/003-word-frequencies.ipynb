{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "584fce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "from pathlib import Path as pth\n",
    "import subprocess as sp\n",
    "import warnings\n",
    "from typing import List, Tuple, Any, Dict\n",
    "from itertools import chain\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot  as plt\n",
    "import seaborn  as sns\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ab5efb",
   "metadata": {},
   "source": [
    "## Change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "379d15d3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "os.chdir(pth(pth.home() / 'dev/sms-analysis'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd5cd2b",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9c3a815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date_time",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "from_me",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sender",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "e394e4cd-ab31-4fdf-9c2d-14648a4aebe9",
       "rows": [
        [
         "0",
         "2021-03-11 17:25:57",
         "So when is a good time? ",
         "1",
         "13608307613"
        ],
        [
         "1",
         "2021-03-11 17:26:37",
         "Tuesday’s I’m free after 1130",
         "1",
         "13608307613"
        ],
        [
         "2",
         "2021-03-11 23:51:13",
         "this is Christian right? ",
         "0",
         "13608307613"
        ],
        [
         "3",
         "2021-03-11 23:52:07",
         "im done at 3 on tuesday’s so anytime after that works for me!",
         "0",
         "13608307613"
        ],
        [
         "4",
         "2021-03-12 09:17:13",
         "Sounds  like a plan",
         "1",
         "13608307613"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>text</th>\n",
       "      <th>from_me</th>\n",
       "      <th>sender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-11 17:25:57</td>\n",
       "      <td>So when is a good time?</td>\n",
       "      <td>1</td>\n",
       "      <td>13608307613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-11 17:26:37</td>\n",
       "      <td>Tuesday’s I’m free after 1130</td>\n",
       "      <td>1</td>\n",
       "      <td>13608307613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-11 23:51:13</td>\n",
       "      <td>this is Christian right?</td>\n",
       "      <td>0</td>\n",
       "      <td>13608307613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-11 23:52:07</td>\n",
       "      <td>im done at 3 on tuesday’s so anytime after tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>13608307613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-12 09:17:13</td>\n",
       "      <td>Sounds  like a plan</td>\n",
       "      <td>1</td>\n",
       "      <td>13608307613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date_time                                               text  \\\n",
       "0  2021-03-11 17:25:57                           So when is a good time?    \n",
       "1  2021-03-11 17:26:37                      Tuesday’s I’m free after 1130   \n",
       "2  2021-03-11 23:51:13                          this is Christian right?    \n",
       "3  2021-03-11 23:52:07  im done at 3 on tuesday’s so anytime after tha...   \n",
       "4  2021-03-12 09:17:13                                Sounds  like a plan   \n",
       "\n",
       "   from_me       sender  \n",
       "0        1  13608307613  \n",
       "1        1  13608307613  \n",
       "2        0  13608307613  \n",
       "3        0  13608307613  \n",
       "4        1  13608307613  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = pd.read_csv('processed-data/X-messages.csv')\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbdfc9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenize messages by sender\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def tokenize(text):\n",
    "    return [t.text.lower() for t in nlp(text) if t.is_alpha]\n",
    "\n",
    "messages = (\n",
    "    messages\n",
    "        .dropna(subset=[\"text\"])\n",
    "        .assign(tokens=lambda df: df[\"text\"].apply(tokenize))\n",
    ")\n",
    "\n",
    "# # Filter out stop words\n",
    "# stop_words = set(STOP_WORDS)\n",
    "# messages[\"tokens\"] = messages[\"tokens\"].apply(\n",
    "#     lambda toks: [t for t in toks if t not in stop_words]\n",
    "# )\n",
    "\n",
    "\n",
    "# filter for non_empty tokens\n",
    "messages = messages[messages.tokens.apply(len) != 0]\n",
    "\n",
    "\n",
    "my_tokens = (\n",
    "    messages[\"tokens\"][messages.from_me==1]\n",
    "    .explode()\n",
    "    .rename(\"token\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "their_tokens = (\n",
    "    messages[\"tokens\"][messages.from_me==0]\n",
    "    .explode()\n",
    "    .rename(\"token\")\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "887c720c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "token",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "ccb47d75-a581-4052-82d0-d7f3412a9102",
       "rows": [
        [
         "i",
         "2771"
        ],
        [
         "you",
         "2054"
        ],
        [
         "to",
         "1405"
        ],
        [
         "the",
         "857"
        ],
        [
         "and",
         "743"
        ],
        [
         "it",
         "716"
        ],
        [
         "a",
         "711"
        ],
        [
         "so",
         "619"
        ],
        [
         "at",
         "513"
        ],
        [
         "my",
         "508"
        ],
        [
         "that",
         "480"
        ],
        [
         "for",
         "440"
        ],
        [
         "have",
         "436"
        ],
        [
         "in",
         "403"
        ],
        [
         "can",
         "400"
        ],
        [
         "do",
         "389"
        ],
        [
         "we",
         "386"
        ],
        [
         "be",
         "359"
        ],
        [
         "just",
         "357"
        ],
        [
         "love",
         "355"
        ],
        [
         "is",
         "354"
        ],
        [
         "me",
         "350"
        ],
        [
         "if",
         "349"
        ],
        [
         "loved",
         "323"
        ],
        [
         "are",
         "309"
        ],
        [
         "this",
         "293"
        ],
        [
         "want",
         "281"
        ],
        [
         "your",
         "280"
        ],
        [
         "too",
         "276"
        ],
        [
         "but",
         "271"
        ],
        [
         "on",
         "265"
        ],
        [
         "of",
         "263"
        ],
        [
         "will",
         "255"
        ],
        [
         "was",
         "254"
        ],
        [
         "okay",
         "251"
        ],
        [
         "with",
         "228"
        ],
        [
         "good",
         "226"
        ],
        [
         "up",
         "200"
        ],
        [
         "about",
         "194"
        ],
        [
         "yes",
         "193"
        ],
        [
         "would",
         "191"
        ],
        [
         "out",
         "184"
        ],
        [
         "like",
         "172"
        ],
        [
         "get",
         "164"
        ],
        [
         "not",
         "154"
        ],
        [
         "did",
         "153"
        ],
        [
         "going",
         "152"
        ],
        [
         "still",
         "148"
        ],
        [
         "also",
         "147"
        ],
        [
         "go",
         "146"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 3593
       }
      },
      "text/plain": [
       "token\n",
       "i           2771\n",
       "you         2054\n",
       "to          1405\n",
       "the          857\n",
       "and          743\n",
       "            ... \n",
       "timeline       1\n",
       "solidify       1\n",
       "fishing        1\n",
       "tube           1\n",
       "hitting        1\n",
       "Name: count, Length: 3593, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tokens.value_counts().sort_values( ascending = False)\n",
    "their_tokens.value_counts().sort_values( ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9115c26d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# patt = re.compile(r'love')\n",
    "their_token_total = their_tokens.shape[0]\n",
    "my_token_total = my_tokens.shape[0]\n",
    "\n",
    "def print_word_count(pattern: str):\n",
    "    print(pattern)\n",
    "    \n",
    "    their_count = (their_tokens.str.contains(pattern, case=False)).sum()\n",
    "    my_count = (my_tokens.str.contains(pattern, case=False)).sum()\n",
    "\n",
    "    my_prop = round(100*my_count/my_token_total, 2)\n",
    "    their_prop = round(100*their_count/their_token_total, 2)\n",
    "\n",
    "    print(\"  Them: \", their_count, f'({their_prop}%)')\n",
    "    print(\"  Me: \", my_count, f'({my_prop}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "830b7573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "  Them:  715 (1.63%)\n",
      "  Me:  579 (2.05%)\n",
      "like\n",
      "  Them:  249 (0.57%)\n",
      "  Me:  287 (1.02%)\n",
      "happy\n",
      "  Them:  31 (0.07%)\n",
      "  Me:  21 (0.07%)\n",
      "sad\n",
      "  Them:  20 (0.05%)\n",
      "  Me:  12 (0.04%)\n",
      "miss\n",
      "  Them:  105 (0.24%)\n",
      "  Me:  118 (0.42%)\n",
      "wish\n",
      "  Them:  16 (0.04%)\n",
      "  Me:  27 (0.1%)\n",
      "hat\n",
      "  Them:  654 (1.49%)\n",
      "  Me:  472 (1.67%)\n",
      "dog\n",
      "  Them:  4 (0.01%)\n",
      "  Me:  3 (0.01%)\n",
      "mad\n",
      "  Them:  39 (0.09%)\n",
      "  Me:  27 (0.1%)\n",
      "time\n",
      "  Them:  171 (0.39%)\n",
      "  Me:  77 (0.27%)\n",
      "plan\n",
      "  Them:  78 (0.18%)\n",
      "  Me:  40 (0.14%)\n",
      "you\n",
      "  Them:  2348 (5.35%)\n",
      "  Me:  1639 (5.8%)\n",
      "me\n",
      "  Them:  1249 (2.84%)\n",
      "  Me:  832 (2.94%)\n",
      "how|why|where|when\n",
      "  Them:  363 (0.83%)\n",
      "  Me:  331 (1.17%)\n"
     ]
    }
   ],
   "source": [
    "patterns = ['love', 'like', 'happy', 'sad', 'miss', 'wish',  \n",
    "            'hat', 'dog', 'mad', 'time', 'plan', 'you', 'me', \n",
    "            'how|why|where|when']\n",
    "for patt in patterns:\n",
    "    print_word_count(patt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd096c86",
   "metadata": {},
   "source": [
    "## use LWIC dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0578741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download dictionary\n",
    "if not pth.exists(pth(\"raw-data/LIWC2007.dic\")):\n",
    "    sp.run([\n",
    "        'curl',\n",
    "        '-L',\n",
    "        'https://raw.githubusercontent.com/Harsh-Panchal-1403/LIWC_PROJECT/master/LIWC2007_English100131.dic',\n",
    "        '-o',\n",
    "        'raw-data/LIWC2007.dic'\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2c1f06",
   "metadata": {},
   "source": [
    "\n",
    "### Read in dictionary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec43e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def read_dic(path: str) -> Dict[re.Pattern, list[str]]:\n",
    "#     with open(path, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "#     # find lines with field names\n",
    "#     key_marker = [s.strip() == '%' for s in lines]\n",
    "#     key_marker_indx = np.where(key_marker)[0]\n",
    "\n",
    "#     # save keys as dict of key: refnum\n",
    "#     full_keys = lines[key_marker_indx[0] + 1 : key_marker_indx[1] ]\n",
    "#     values = lines[key_marker_indx[1]+1:]\n",
    "\n",
    "#     ref_num_dict = {}\n",
    "#     pat = re.compile(r'^(\\d+)\\t(.+)')\n",
    "#     for s in full_keys:\n",
    "#         m = pat.search(s)\n",
    "#         if m:\n",
    "#             ref_num_dict[int(m.group(1))] = m.group(2).strip()\n",
    "\n",
    "#     # save strings as dict string: refnum\n",
    "#     string_dict = {}\n",
    "#     # val = \"sdlfkj   130294  13294   130459\"\n",
    "#     for val in values:\n",
    "#         refs = [int(x) for x in re.findall(r'\\d+', val)]\n",
    "#         string = val.split('\\t', 1)[0].strip()\n",
    "#         string = re.sub(pattern=r'\\s+',repl= '', string=string)\n",
    "#         string_dict.setdefault(string, []).extend(refs)\n",
    "\n",
    "#     # make one unified dict with key: strings\n",
    "#     full_dict = {}\n",
    "#     for string, ref_nums in string_dict.items():\n",
    "#         cats = []\n",
    "#         for ref_num in ref_nums:\n",
    "#             if ref_num in ref_num_dict:\n",
    "#                 cats.append(ref_num_dict[ref_num])\n",
    "#         string = re.compile(\"^\" + re.escape(string).replace(r\"\\*\", \".*\") + \"$\")\n",
    "#         full_dict[string] = cats\n",
    "\n",
    "#     return full_dict, [re.sub(r'\\d+\\t|\\n', '', x) for x in full_keys]\n",
    "\n",
    "\n",
    "# # Read dic\n",
    "# dic, categories = read_dic(\"raw-data/LIWC2007.dic\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f9bcea",
   "metadata": {},
   "source": [
    "\n",
    "### Map Texts to Categories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2425ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # return categories for word\n",
    "# def get_categories(tokens) :\n",
    "#     res = []\n",
    "#     for tok in tokens:\n",
    "#         res.append([\n",
    "#             cat\n",
    "#             for patt, cats in dic.items()\n",
    "#             if patt.match(tok)\n",
    "#             for cat in cats\n",
    "#         ])\n",
    "\n",
    "#     # category x message series\n",
    "#     res = pd.Series(res).explode().value_counts()\n",
    "\n",
    "#     # add missing categories\n",
    "#     if not res.shape[0] == len(categories) :\n",
    "#         diff = set(categories).difference(set(res.index))\n",
    "#         add = pd.Series(0,  index = list(diff))\n",
    "#         res = pd.concat([res, add], axis = 0)\n",
    "\n",
    "#     res = res.sort_index()\n",
    "\n",
    "#     return res\n",
    "\n",
    "# #%%\n",
    "# # get_categories( messages.loc[1,'tokens'])\n",
    "\n",
    "\n",
    "# # Get categories for each message as vector of category counts\n",
    "# category_counts = pd.DataFrame([get_categories(x) for x in messages.tokens])\n",
    "# category_counts.shape\n",
    "# messages.shape\n",
    "\n",
    "# messages.reset_index(drop = True, inplace = True)\n",
    "# category_counts.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# messages.index\n",
    "# messages.columns\n",
    "# category_counts.index\n",
    "# category_counts.columns\n",
    "\n",
    "# x = pd.concat([messages, category_counts], axis = 1)\n",
    "\n",
    "# if x.shape[0] == messages.shape[0]:\n",
    "#     messages = x\n",
    "# else:\n",
    "#     warnings.warn(\n",
    "#         \"Row count mismatch: x does not match messages; assignment skipped.\",\n",
    "#         UserWarning\n",
    "#     )\n",
    "\n",
    "# #%% [markdown]\n",
    "# # ## Compare category counts\n",
    "\n",
    "# #%%\n",
    "# my_cat_counts = messages.loc[messages['from_me']==1,'achieve':].sum(axis = 0)\n",
    "# their_cat_counts = messages.loc[messages['from_me']==0,'achieve':].sum(axis = 0)\n",
    "\n",
    "# # normalize counts\n",
    "# # total_cat_counts = sum(my_cat_counts, their_cat_counts)\n",
    "# my_cat_freq = my_cat_counts/my_cat_counts.sum(0)\n",
    "# their_cat_freq = their_cat_counts/their_cat_counts.sum(0)\n",
    "\n",
    "# cat_freq_summary = pd.concat([my_cat_freq, their_cat_freq], axis = 1).rename(columns={0: \"me\", 1: \"them\"})\n",
    "\n",
    "# cat_freq_summary['me_over_them'] = cat_freq_summary['me'].div(cat_freq_summary['them'], axis = 0).round(4)\n",
    "# cat_freq_summary['log_me_over_them'] = np.log2(cat_freq_summary['me_over_them'])\n",
    "\n",
    "# # add patterns to df\n",
    "# cat_to_patterns = defaultdict(list)\n",
    "# for patt, cats in dic.items():\n",
    "#     for cat in cats:\n",
    "#         cat_to_patterns[cat].append(patt.pattern)\n",
    "\n",
    "# cat_freq_summary[\"patterns\"] = (\n",
    "#     cat_freq_summary.index.map(lambda c: cat_to_patterns.get(c, []))\n",
    "# )\n",
    "\n",
    "\n",
    "# # print summary\n",
    "# (\n",
    "#     cat_freq_summary\n",
    "#     .sort_values(by=\"log_me_over_them\",key=lambda s: s.abs(), ascending = False)\n",
    "#     .to_csv('results/summary.txt', sep='\\t')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad06d1ba",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Use ConvoKit tool\n",
    "Jonathan P. Chang, Caleb Chiam, Liye Fu, Andrew Wang, Justine Zhang, Cristian Danescu-Niculescu-Mizil. 2020. \"ConvoKit: A Toolkit for the Analysis of Conversations\". Proceedings of SIGDIAL.\n",
    "\n",
    "  1. Download the toolkit: pip3 install convokit\n",
    "  2. Download Spacy's English model: python3 -m spacy download en\n",
    "  3. Download NLTK's 'punkt' model: import nltk; nltk.download('punkt') (in Python interpreter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d90c917",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/canderson/miniconda3/envs/generic-python/lib/python3.12/site-packages/convokit/coordination/coordination.py:5: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "[nltk_data] Downloading package punkt to /Users/canderson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from convokit import Corpus, Utterance, Speaker, TextParser, Coordination,PolitenessStrategies\n",
    "import nltk; nltk.download('punkt')\n",
    "# spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239d65dd",
   "metadata": {},
   "source": [
    "#### Construct corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8633b55",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "messages.date_time = pd.to_datetime(messages.date_time)\n",
    "df = messages.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05fccb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def speaker_id(row):\n",
    "    return \"me\" if row[\"from_me\"] == 1 else \"them\"\n",
    "# make speakers\n",
    "speakers = {\n",
    "    \"me\": Speaker(id=\"me\"),\n",
    "    \"them\": Speaker(id=\"them\")\n",
    "}\n",
    "utterances = []\n",
    "conversation_id = \"sms_conversation_1\"\n",
    "prev_utt_id = None\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    utt_id = f\"utt_{i}\"\n",
    "\n",
    "    utt = Utterance(\n",
    "        id=utt_id,\n",
    "        speaker=speakers[speaker_id(row)],\n",
    "        text=row[\"text\"],\n",
    "        reply_to=prev_utt_id,\n",
    "        conversation_id=conversation_id,\n",
    "        meta={\n",
    "            \"timestamp\": row[\"date_time\"].isoformat(),\n",
    "            \"from_me\": row[\"from_me\"],\n",
    "            \"sender\": row[\"sender\"]\n",
    "            # \"tokens\": row[\"tokens\"]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    utterances.append(utt)\n",
    "    prev_utt_id = utt_id\n",
    "\n",
    "corpus = Corpus(\n",
    "    utterances=utterances\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a8bc383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['add_meta',\n",
       " 'add_utterances',\n",
       " 'append_vector_matrix',\n",
       " 'backend',\n",
       " 'backend_mapper',\n",
       " 'config',\n",
       " 'conversations',\n",
       " 'corpus_dirpath',\n",
       " 'delete_metadata',\n",
       " 'delete_vector_matrix',\n",
       " 'directed_pairwise_exchanges',\n",
       " 'dump',\n",
       " 'dump_info',\n",
       " 'dump_vectors',\n",
       " 'filter_conversations_by',\n",
       " 'filter_utterances',\n",
       " 'from_pandas',\n",
       " 'get_attribute_table',\n",
       " 'get_conversation',\n",
       " 'get_conversation_ids',\n",
       " 'get_conversations_dataframe',\n",
       " 'get_full_attribute_table',\n",
       " 'get_meta',\n",
       " 'get_object',\n",
       " 'get_object_ids',\n",
       " 'get_speaker',\n",
       " 'get_speaker_convo_attribute_table',\n",
       " 'get_speaker_convo_info',\n",
       " 'get_speaker_ids',\n",
       " 'get_speakers_dataframe',\n",
       " 'get_utterance',\n",
       " 'get_utterance_ids',\n",
       " 'get_utterances_dataframe',\n",
       " 'get_vector_matrix',\n",
       " 'get_vectors',\n",
       " 'has_conversation',\n",
       " 'has_speaker',\n",
       " 'has_utterance',\n",
       " 'id',\n",
       " 'iter_conversations',\n",
       " 'iter_objs',\n",
       " 'iter_speakers',\n",
       " 'iter_utterances',\n",
       " 'load_info',\n",
       " 'merge',\n",
       " 'meta',\n",
       " 'meta_index',\n",
       " 'organize_speaker_convo_history',\n",
       " 'print_summary_stats',\n",
       " 'random_conversation',\n",
       " 'random_speaker',\n",
       " 'random_utterance',\n",
       " 'reconnect_to_db',\n",
       " 'reindex_conversations',\n",
       " 'reinitialize_index',\n",
       " 'set_speaker_convo_info',\n",
       " 'set_vector_matrix',\n",
       " 'speakers',\n",
       " 'speaking_pairs',\n",
       " 'update_metadata_from_df',\n",
       " 'update_speakers_data',\n",
       " 'utterances',\n",
       " 'vectors']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x  in dir(corpus) if not bool(re.search(r'^_', x)) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5c725e",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77bf37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = TextParser('en_core_web_sm')\n",
    "corpus = parser.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7483d056",
   "metadata": {},
   "source": [
    "### Analyze Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5150e82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Speakers: 2\n",
      "Number of Utterances: 9281\n",
      "Number of Conversations: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['So', 'when', 'is', 'a', 'good', 'time', '?']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.print_summary_stats()\n",
    "\n",
    "[t[\"tok\"] for sent in corpus.get_utterance(\"utt_0\").meta[\"en_core_web_sm\"] for t in sent[\"toks\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c07db46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speakers in corpus: [Speaker({'obj_type': 'speaker', 'vectors': [], 'owner': <convokit.model.corpus.Corpus object at 0x176d84b90>, 'id': 'me', 'meta': ConvoKitMeta({})}), Speaker({'obj_type': 'speaker', 'vectors': [], 'owner': <convokit.model.corpus.Corpus object at 0x176d84b90>, 'id': 'them', 'meta': ConvoKitMeta({})})]\n",
      "{('me', 'them'), ('them', 'them'), ('me', 'me'), ('them', 'me')}\n"
     ]
    }
   ],
   "source": [
    "print(\"Speakers in corpus:\", list(corpus.iter_speakers()))  \n",
    "print(corpus.speaking_pairs(speaker_ids_only=True)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad677e2f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Speaker Coordination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0d697",
   "metadata": {},
   "source": [
    "|Feature|me_to_them|them_to_me|Interpretation|\n",
    "|---|---|---|---|\n",
    "|auxverb|0.31|0.05|You strongly accommodate their auxiliary verbs; they barely adapt to yours|\n",
    "|pronoun|−0.02|0.21|You slightly diverge; they strongly accommodate|\n",
    "|article|0.00|0.00|No coordination either way|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0547877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speaker coordination\n",
    "coord = Coordination(target_thresh=3, speaker_thresh=5, utterances_thresh=5)  \n",
    "\n",
    "coord.fit(corpus)  \n",
    "\n",
    "coord.transform(corpus)\n",
    "\n",
    "me_coord_scores = corpus.get_speaker(\"me\").meta[\"coord\"]['them']\n",
    "them_coord_scores = corpus.get_speaker(\"them\").meta[\"coord\"]['me']\n",
    "feature_freqs = pd.concat([pd.Series(me_coord_scores).rename(\"me_to_them\"), pd.Series(them_coord_scores).rename(\"them_to_me\")], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5a3c180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "me_to_them",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "them_to_me",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "diff",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "e12e051b-f2fd-4115-a053-576879fd066b",
       "rows": [
        [
         "quant",
         "0.12296232573595407",
         "0.15329576084293065",
         "-0.030333435106976572"
        ],
        [
         "adverb",
         "0.06646424557110603",
         "0.0967975768654753",
         "-0.03033333129436927"
        ],
        [
         "ipron",
         "0.12097393026317271",
         "0.15093128198915778",
         "-0.02995735172598507"
        ],
        [
         "ppron",
         "0.04013340090418216",
         "0.06932452347015194",
         "-0.029191122565969785"
        ],
        [
         "auxverb",
         "0.047595301418439706",
         "0.06921504731300138",
         "-0.02161974589456167"
        ],
        [
         "preps",
         "0.053647900186662545",
         "0.0660907205014683",
         "-0.012442820314805758"
        ],
        [
         "conj",
         "0.0962389673566208",
         "0.10440592918768082",
         "-0.008166961831060027"
        ],
        [
         "article",
         "0.07855802068948411",
         "0.0777459040634057",
         "0.000812116626078413"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>me_to_them</th>\n",
       "      <th>them_to_me</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>quant</th>\n",
       "      <td>0.122962</td>\n",
       "      <td>0.153296</td>\n",
       "      <td>-0.030333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adverb</th>\n",
       "      <td>0.066464</td>\n",
       "      <td>0.096798</td>\n",
       "      <td>-0.030333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ipron</th>\n",
       "      <td>0.120974</td>\n",
       "      <td>0.150931</td>\n",
       "      <td>-0.029957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppron</th>\n",
       "      <td>0.040133</td>\n",
       "      <td>0.069325</td>\n",
       "      <td>-0.029191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auxverb</th>\n",
       "      <td>0.047595</td>\n",
       "      <td>0.069215</td>\n",
       "      <td>-0.021620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preps</th>\n",
       "      <td>0.053648</td>\n",
       "      <td>0.066091</td>\n",
       "      <td>-0.012443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conj</th>\n",
       "      <td>0.096239</td>\n",
       "      <td>0.104406</td>\n",
       "      <td>-0.008167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article</th>\n",
       "      <td>0.078558</td>\n",
       "      <td>0.077746</td>\n",
       "      <td>0.000812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         me_to_them  them_to_me      diff\n",
       "quant      0.122962    0.153296 -0.030333\n",
       "adverb     0.066464    0.096798 -0.030333\n",
       "ipron      0.120974    0.150931 -0.029957\n",
       "ppron      0.040133    0.069325 -0.029191\n",
       "auxverb    0.047595    0.069215 -0.021620\n",
       "preps      0.053648    0.066091 -0.012443\n",
       "conj       0.096239    0.104406 -0.008167\n",
       "article    0.078558    0.077746  0.000812"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_freqs['diff'] = feature_freqs['me_to_them']-feature_freqs['them_to_me']\n",
    "\n",
    "feature_freqs.sort_values('diff', key = lambda x: abs(x), ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2241561",
   "metadata": {},
   "source": [
    "### Politeness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0198d4",
   "metadata": {},
   "source": [
    "Politeness features\n",
    "\n",
    "| Feature name              | What it measures | Typical interpretation in discourse analysis |\n",
    "|---------------------------|------------------|----------------------------------------------|\n",
    "| Please                    | Presence of the word “please” anywhere in the utterance | Politeness marker; mitigates imposition |\n",
    "| Please_start              | Utterance begins with “please” | High politeness or deference at turn entry |\n",
    "| HASHEDGE                  | Any hedge expression (aggregate indicator) | Linguistic uncertainty, softening, or non-commitment |\n",
    "| Indirect_(btw)            | Indirect discourse marker such as “by the way” | Topic shift or low-imposition insertion |\n",
    "| Hedges                    | Count or presence of hedging terms (e.g., “maybe”, “kind of”) | Reduced certainty; politeness or epistemic caution |\n",
    "| Factuality                | Use of factual/assertive language | Speaker presents information as objective or certain |\n",
    "| Deference                 | Deferential language (e.g., “if you don’t mind”) | Power asymmetry or respect toward interlocutor |\n",
    "| Gratitude                 | Expressions of thanks | Positive social signaling; rapport maintenance |\n",
    "| Apologizing               | Apologies or regret expressions | Face-saving, repair, or politeness strategy |\n",
    "| 1st_person_pl.            | First-person plural pronouns (“we”, “us”) | Inclusivity, shared responsibility, alignment |\n",
    "| 1st_person                | First-person singular pronouns (“I”, “me”) | Self-focus, agency, or ownership of stance |\n",
    "| 1st_person_start          | Utterance begins with a first-person pronoun | Self-initiated stance or framing |\n",
    "| 2nd_person                | Second-person pronouns (“you”) | Addressing, directing, or engaging the interlocutor |\n",
    "| 2nd_person_start          | Utterance begins with a second-person pronoun | Direct engagement; can signal instruction or confrontation |\n",
    "| Indirect_(greeting)       | Indirect greeting (e.g., “hey”, “hope you’re well”) | Social lubrication before substantive content |\n",
    "| Direct_question           | Explicit interrogative form | Information-seeking or directive questioning |\n",
    "| Direct_start              | Utterance begins with a direct request or statement | Low mitigation; task-oriented or assertive style |\n",
    "| HASPOSITIVE               | Presence of positive-affect words | Positive sentiment or encouragement |\n",
    "| HASNEGATIVE               | Presence of negative-affect words | Criticism, frustration, or negative sentiment |\n",
    "| SUBJUNCTIVE               | Subjunctive or hypothetical constructions (“would”, “could”) | Politeness, mitigation, or counterfactual framing |\n",
    "| INDICATIVE                | Indicative (statement-of-fact) constructions | Assertion, certainty, or declarative stance |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19b9d548",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Initialize politeness analyzer (requires parsed text)  \n",
    "ps = PolitenessStrategies(parse_attribute_name=\"en_core_web_sm\")  \n",
    "corpus= ps.fit_transform(corpus)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ec48d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "me",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "them",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "me_self_normalized",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "them_self_normalized",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "diff",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "776c7cde-ba05-4819-b930-c6372d91ced8",
       "rows": [
        [
         "1st_person",
         "961",
         "1388",
         "0.22122467771639043",
         "0.2811423941664979",
         "-0.05991771645010746"
        ],
        [
         "2nd_person",
         "1218",
         "1651",
         "0.2803867403314917",
         "0.3344136115049625",
         "-0.054026871173470825"
        ],
        [
         "HASPOSITIVE",
         "1694",
         "2155",
         "0.38996316758747696",
         "0.4364998987239214",
         "-0.046536731136444465"
        ],
        [
         "HASHEDGE",
         "404",
         "682",
         "0.09300184162062615",
         "0.13814057119708326",
         "-0.04513872957645711"
        ],
        [
         "1st_person_start",
         "952",
         "1292",
         "0.21915285451197053",
         "0.2616973870771724",
         "-0.042544532565201854"
        ],
        [
         "1st_person_pl.",
         "217",
         "407",
         "0.04995395948434622",
         "0.08243872797245291",
         "-0.03248476848810669"
        ],
        [
         "Direct_start",
         "112",
         "277",
         "0.02578268876611418",
         "0.05610694753899129",
         "-0.030324258772877113"
        ],
        [
         "Indirect_(greeting)",
         "8",
         "109",
         "0.001841620626151013",
         "0.022078185132671662",
         "-0.02023656450652065"
        ],
        [
         "HASNEGATIVE",
         "489",
         "650",
         "0.11256906077348067",
         "0.13165890216730808",
         "-0.01908984139382741"
        ],
        [
         "Gratitude",
         "123",
         "218",
         "0.028314917127071824",
         "0.044156370265343324",
         "-0.0158414531382715"
        ],
        [
         "Hedges",
         "165",
         "263",
         "0.03798342541436464",
         "0.053271217338464655",
         "-0.015287791924100014"
        ],
        [
         "Direct_question",
         "201",
         "165",
         "0.0462707182320442",
         "0.033421105934778206",
         "0.012849612297265993"
        ],
        [
         "Deference",
         "114",
         "70",
         "0.026243093922651933",
         "0.014178651002633179",
         "0.012064442920018754"
        ],
        [
         "Factuality",
         "47",
         "97",
         "0.0108195211786372",
         "0.019647559246505974",
         "-0.008828038067868773"
        ],
        [
         "Apologizing",
         "78",
         "130",
         "0.017955801104972375",
         "0.026331780433461616",
         "-0.008375979328489241"
        ],
        [
         "2nd_person_start",
         "130",
         "168",
         "0.02992633517495396",
         "0.03402876240631963",
         "-0.0041024272313656684"
        ],
        [
         "INDICATIVE",
         "46",
         "67",
         "0.010589318600368325",
         "0.013570994531091757",
         "-0.002981675930723432"
        ],
        [
         "SUBJUNCTIVE",
         "16",
         "31",
         "0.003683241252302026",
         "0.006279116872594693",
         "-0.002595875620292667"
        ],
        [
         "Please_start",
         "10",
         "4",
         "0.0023020257826887663",
         "0.0008102086287218959",
         "0.0014918171539668704"
        ],
        [
         "Please",
         "11",
         "12",
         "0.0025322283609576428",
         "0.0024306258861656878",
         "0.000101602474791955"
        ],
        [
         "Indirect_(btw)",
         "0",
         "0",
         "0.0",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 21
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>me</th>\n",
       "      <th>them</th>\n",
       "      <th>me_self_normalized</th>\n",
       "      <th>them_self_normalized</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1st_person</th>\n",
       "      <td>961</td>\n",
       "      <td>1388</td>\n",
       "      <td>0.221225</td>\n",
       "      <td>0.281142</td>\n",
       "      <td>-0.059918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2nd_person</th>\n",
       "      <td>1218</td>\n",
       "      <td>1651</td>\n",
       "      <td>0.280387</td>\n",
       "      <td>0.334414</td>\n",
       "      <td>-0.054027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HASPOSITIVE</th>\n",
       "      <td>1694</td>\n",
       "      <td>2155</td>\n",
       "      <td>0.389963</td>\n",
       "      <td>0.436500</td>\n",
       "      <td>-0.046537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HASHEDGE</th>\n",
       "      <td>404</td>\n",
       "      <td>682</td>\n",
       "      <td>0.093002</td>\n",
       "      <td>0.138141</td>\n",
       "      <td>-0.045139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st_person_start</th>\n",
       "      <td>952</td>\n",
       "      <td>1292</td>\n",
       "      <td>0.219153</td>\n",
       "      <td>0.261697</td>\n",
       "      <td>-0.042545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st_person_pl.</th>\n",
       "      <td>217</td>\n",
       "      <td>407</td>\n",
       "      <td>0.049954</td>\n",
       "      <td>0.082439</td>\n",
       "      <td>-0.032485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Direct_start</th>\n",
       "      <td>112</td>\n",
       "      <td>277</td>\n",
       "      <td>0.025783</td>\n",
       "      <td>0.056107</td>\n",
       "      <td>-0.030324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indirect_(greeting)</th>\n",
       "      <td>8</td>\n",
       "      <td>109</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>0.022078</td>\n",
       "      <td>-0.020237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HASNEGATIVE</th>\n",
       "      <td>489</td>\n",
       "      <td>650</td>\n",
       "      <td>0.112569</td>\n",
       "      <td>0.131659</td>\n",
       "      <td>-0.019090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gratitude</th>\n",
       "      <td>123</td>\n",
       "      <td>218</td>\n",
       "      <td>0.028315</td>\n",
       "      <td>0.044156</td>\n",
       "      <td>-0.015841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hedges</th>\n",
       "      <td>165</td>\n",
       "      <td>263</td>\n",
       "      <td>0.037983</td>\n",
       "      <td>0.053271</td>\n",
       "      <td>-0.015288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Direct_question</th>\n",
       "      <td>201</td>\n",
       "      <td>165</td>\n",
       "      <td>0.046271</td>\n",
       "      <td>0.033421</td>\n",
       "      <td>0.012850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deference</th>\n",
       "      <td>114</td>\n",
       "      <td>70</td>\n",
       "      <td>0.026243</td>\n",
       "      <td>0.014179</td>\n",
       "      <td>0.012064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Factuality</th>\n",
       "      <td>47</td>\n",
       "      <td>97</td>\n",
       "      <td>0.010820</td>\n",
       "      <td>0.019648</td>\n",
       "      <td>-0.008828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apologizing</th>\n",
       "      <td>78</td>\n",
       "      <td>130</td>\n",
       "      <td>0.017956</td>\n",
       "      <td>0.026332</td>\n",
       "      <td>-0.008376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2nd_person_start</th>\n",
       "      <td>130</td>\n",
       "      <td>168</td>\n",
       "      <td>0.029926</td>\n",
       "      <td>0.034029</td>\n",
       "      <td>-0.004102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDICATIVE</th>\n",
       "      <td>46</td>\n",
       "      <td>67</td>\n",
       "      <td>0.010589</td>\n",
       "      <td>0.013571</td>\n",
       "      <td>-0.002982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUBJUNCTIVE</th>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>0.003683</td>\n",
       "      <td>0.006279</td>\n",
       "      <td>-0.002596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Please_start</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.001492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Please</th>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.002431</td>\n",
       "      <td>0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indirect_(btw)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       me  them  me_self_normalized  them_self_normalized  \\\n",
       "1st_person            961  1388            0.221225              0.281142   \n",
       "2nd_person           1218  1651            0.280387              0.334414   \n",
       "HASPOSITIVE          1694  2155            0.389963              0.436500   \n",
       "HASHEDGE              404   682            0.093002              0.138141   \n",
       "1st_person_start      952  1292            0.219153              0.261697   \n",
       "1st_person_pl.        217   407            0.049954              0.082439   \n",
       "Direct_start          112   277            0.025783              0.056107   \n",
       "Indirect_(greeting)     8   109            0.001842              0.022078   \n",
       "HASNEGATIVE           489   650            0.112569              0.131659   \n",
       "Gratitude             123   218            0.028315              0.044156   \n",
       "Hedges                165   263            0.037983              0.053271   \n",
       "Direct_question       201   165            0.046271              0.033421   \n",
       "Deference             114    70            0.026243              0.014179   \n",
       "Factuality             47    97            0.010820              0.019648   \n",
       "Apologizing            78   130            0.017956              0.026332   \n",
       "2nd_person_start      130   168            0.029926              0.034029   \n",
       "INDICATIVE             46    67            0.010589              0.013571   \n",
       "SUBJUNCTIVE            16    31            0.003683              0.006279   \n",
       "Please_start           10     4            0.002302              0.000810   \n",
       "Please                 11    12            0.002532              0.002431   \n",
       "Indirect_(btw)          0     0            0.000000              0.000000   \n",
       "\n",
       "                         diff  \n",
       "1st_person          -0.059918  \n",
       "2nd_person          -0.054027  \n",
       "HASPOSITIVE         -0.046537  \n",
       "HASHEDGE            -0.045139  \n",
       "1st_person_start    -0.042545  \n",
       "1st_person_pl.      -0.032485  \n",
       "Direct_start        -0.030324  \n",
       "Indirect_(greeting) -0.020237  \n",
       "HASNEGATIVE         -0.019090  \n",
       "Gratitude           -0.015841  \n",
       "Hedges              -0.015288  \n",
       "Direct_question      0.012850  \n",
       "Deference            0.012064  \n",
       "Factuality          -0.008828  \n",
       "Apologizing         -0.008376  \n",
       "2nd_person_start    -0.004102  \n",
       "INDICATIVE          -0.002982  \n",
       "SUBJUNCTIVE         -0.002596  \n",
       "Please_start         0.001492  \n",
       "Please               0.000102  \n",
       "Indirect_(btw)       0.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  \n",
    "# Get politeness scores for each speaker's utterances  \n",
    "me_utterances = list(corpus.iter_utterances(lambda x: x.speaker.id == \"me\"))  \n",
    "them_utterances = list(corpus.iter_utterances(lambda x: x.speaker.id == \"them\"))  \n",
    "  \n",
    "# Calculate average politeness strategies per speaker  \n",
    "me_strategies = pd.DataFrame([utt.meta[\"politeness_strategies\"] for utt in me_utterances])\n",
    "them_strategies = pd.DataFrame([utt.meta[\"politeness_strategies\"] for utt in them_utterances])\n",
    "\n",
    "out = pd.concat([me_strategies.sum(0), them_strategies.sum(0)],axis = 1)\n",
    "out.columns = ['me', 'them']\n",
    "out.index = [re.sub(pattern = r'feature_politeness_|==',repl = '', string= x) for x in out.index]\n",
    "\n",
    "# normalize by total utterances spoken\n",
    "utt_counts = Counter(\n",
    "    utt.speaker.id\n",
    "    for utt in corpus.iter_utterances()\n",
    ")\n",
    "\n",
    "out['me_self_normalized'] = out.me/utt_counts['me']\n",
    "out['them_self_normalized'] = out.them/utt_counts['them']\n",
    "\n",
    "out['diff'] = out.me_self_normalized-out.them_self_normalized\n",
    "\n",
    "out.sort_values('diff', key = lambda x: abs(x), ascending = False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/Users/canderson/miniconda3/envs/generic-python/bin/python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "generic-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
