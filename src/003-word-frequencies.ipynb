{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584fce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "from pathlib import Path as pth\n",
    "import subprocess as sp\n",
    "import warnings\n",
    "from typing import List, Tuple, Any, Dict\n",
    "from itertools import chain\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot  as plt\n",
    "import seaborn  as sns\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ab5efb",
   "metadata": {},
   "source": [
    "## Change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d15d3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "os.chdir(pth(pth.home() / 'dev/sms-analysis'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd5cd2b",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c3a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = pd.read_csv('processed-data/X-messages.csv')\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdfc9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenize messages by sender\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def tokenize(text):\n",
    "    return [t.text.lower() for t in nlp(text) if t.is_alpha]\n",
    "\n",
    "messages = (\n",
    "    messages\n",
    "        .dropna(subset=[\"text\"])\n",
    "        .assign(tokens=lambda df: df[\"text\"].apply(tokenize))\n",
    ")\n",
    "\n",
    "# # Filter out stop words\n",
    "# stop_words = set(STOP_WORDS)\n",
    "# messages[\"tokens\"] = messages[\"tokens\"].apply(\n",
    "#     lambda toks: [t for t in toks if t not in stop_words]\n",
    "# )\n",
    "\n",
    "\n",
    "# filter for non_empty tokens\n",
    "messages = messages[messages.tokens.apply(len) != 0]\n",
    "\n",
    "\n",
    "my_tokens = (\n",
    "    messages[\"tokens\"][messages.from_me==1]\n",
    "    .explode()\n",
    "    .rename(\"token\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "their_tokens = (\n",
    "    messages[\"tokens\"][messages.from_me==0]\n",
    "    .explode()\n",
    "    .rename(\"token\")\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8886ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887c720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokens.value_counts().sort_values( ascending = False)\n",
    "their_tokens.value_counts().sort_values( ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9115c26d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# patt = re.compile(r'love')\n",
    "their_token_total = their_tokens.shape[0]\n",
    "my_token_total = my_tokens.shape[0]\n",
    "\n",
    "def print_word_count(pattern: str):\n",
    "    print(pattern)\n",
    "    \n",
    "    their_count = (their_tokens.str.contains(pattern, case=False)).sum()\n",
    "    my_count = (my_tokens.str.contains(pattern, case=False)).sum()\n",
    "\n",
    "    my_prop = round(100*my_count/my_token_total, 2)\n",
    "    their_prop = round(100*their_count/their_token_total, 2)\n",
    "\n",
    "    print(\"  Them: \", their_count, f'({their_prop}%)')\n",
    "    print(\"  Me: \", my_count, f'({my_prop}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830b7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = ['love', 'like', 'happy', 'sad', 'miss', 'wish',  \n",
    "            'hat', 'dog', 'mad', 'time', 'plan', 'you', 'me', \n",
    "            'how|why|where|when']\n",
    "for patt in patterns:\n",
    "    print_word_count(patt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd096c86",
   "metadata": {},
   "source": [
    "## use LWIC dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0578741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download dictionary\n",
    "if not pth.exists(pth(\"raw-data/LIWC2007.dic\")):\n",
    "    sp.run([\n",
    "        'curl',\n",
    "        '-L',\n",
    "        'https://raw.githubusercontent.com/Harsh-Panchal-1403/LIWC_PROJECT/master/LIWC2007_English100131.dic',\n",
    "        '-o',\n",
    "        'raw-data/LIWC2007.dic'\n",
    "    ])\n",
    "\n",
    "# #%% [markdown]\n",
    "# # ### Read in dictionary \n",
    "\n",
    "# #%%\n",
    "# def read_dic(path: str) -> Dict[re.Pattern, list[str]]:\n",
    "#     with open(path, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "#     # find lines with field names\n",
    "#     key_marker = [s.strip() == '%' for s in lines]\n",
    "#     key_marker_indx = np.where(key_marker)[0]\n",
    "\n",
    "#     # save keys as dict of key: refnum\n",
    "#     full_keys = lines[key_marker_indx[0] + 1 : key_marker_indx[1] ]\n",
    "#     values = lines[key_marker_indx[1]+1:]\n",
    "\n",
    "#     ref_num_dict = {}\n",
    "#     pat = re.compile(r'^(\\d+)\\t(.+)')\n",
    "#     for s in full_keys:\n",
    "#         m = pat.search(s)\n",
    "#         if m:\n",
    "#             ref_num_dict[int(m.group(1))] = m.group(2).strip()\n",
    "\n",
    "#     # save strings as dict string: refnum\n",
    "#     string_dict = {}\n",
    "#     # val = \"sdlfkj   130294  13294   130459\"\n",
    "#     for val in values:\n",
    "#         refs = [int(x) for x in re.findall(r'\\d+', val)]\n",
    "#         string = val.split('\\t', 1)[0].strip()\n",
    "#         string = re.sub(pattern=r'\\s+',repl= '', string=string)\n",
    "#         string_dict.setdefault(string, []).extend(refs)\n",
    "\n",
    "#     # make one unified dict with key: strings\n",
    "#     full_dict = {}\n",
    "#     for string, ref_nums in string_dict.items():\n",
    "#         cats = []\n",
    "#         for ref_num in ref_nums:\n",
    "#             if ref_num in ref_num_dict:\n",
    "#                 cats.append(ref_num_dict[ref_num])\n",
    "#         string = re.compile(\"^\" + re.escape(string).replace(r\"\\*\", \".*\") + \"$\")\n",
    "#         full_dict[string] = cats\n",
    "\n",
    "#     return full_dict, [re.sub(r'\\d+\\t|\\n', '', x) for x in full_keys]\n",
    "\n",
    "# #%% \n",
    "\n",
    "# # Read dic\n",
    "# dic, categories = read_dic(\"raw-data/LIWC2007.dic\")\n",
    "\n",
    "\n",
    "# #%% [markdown]\n",
    "# # ### Map Texts to Categories\n",
    "\n",
    "# #%%\n",
    "\n",
    "# # return categories for word\n",
    "# def get_categories(tokens) :\n",
    "#     res = []\n",
    "#     for tok in tokens:\n",
    "#         res.append([\n",
    "#             cat\n",
    "#             for patt, cats in dic.items()\n",
    "#             if patt.match(tok)\n",
    "#             for cat in cats\n",
    "#         ])\n",
    "\n",
    "#     # category x message series\n",
    "#     res = pd.Series(res).explode().value_counts()\n",
    "\n",
    "#     # add missing categories\n",
    "#     if not res.shape[0] == len(categories) :\n",
    "#         diff = set(categories).difference(set(res.index))\n",
    "#         add = pd.Series(0,  index = list(diff))\n",
    "#         res = pd.concat([res, add], axis = 0)\n",
    "\n",
    "#     res = res.sort_index()\n",
    "\n",
    "#     return res\n",
    "\n",
    "# #%%\n",
    "# # get_categories( messages.loc[1,'tokens'])\n",
    "\n",
    "# #%% \n",
    "\n",
    "# # Get categories for each message as vector of category counts\n",
    "# category_counts = pd.DataFrame([get_categories(x) for x in messages.tokens])\n",
    "# category_counts.shape\n",
    "# messages.shape\n",
    "\n",
    "# messages.reset_index(drop = True, inplace = True)\n",
    "# category_counts.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# messages.index\n",
    "# messages.columns\n",
    "# category_counts.index\n",
    "# category_counts.columns\n",
    "\n",
    "# x = pd.concat([messages, category_counts], axis = 1)\n",
    "\n",
    "# if x.shape[0] == messages.shape[0]:\n",
    "#     messages = x\n",
    "# else:\n",
    "#     warnings.warn(\n",
    "#         \"Row count mismatch: x does not match messages; assignment skipped.\",\n",
    "#         UserWarning\n",
    "#     )\n",
    "\n",
    "# #%% [markdown]\n",
    "# # ## Compare category counts\n",
    "\n",
    "# #%%\n",
    "# my_cat_counts = messages.loc[messages['from_me']==1,'achieve':].sum(axis = 0)\n",
    "# their_cat_counts = messages.loc[messages['from_me']==0,'achieve':].sum(axis = 0)\n",
    "\n",
    "# # normalize counts\n",
    "# # total_cat_counts = sum(my_cat_counts, their_cat_counts)\n",
    "# my_cat_freq = my_cat_counts/my_cat_counts.sum(0)\n",
    "# their_cat_freq = their_cat_counts/their_cat_counts.sum(0)\n",
    "\n",
    "# cat_freq_summary = pd.concat([my_cat_freq, their_cat_freq], axis = 1).rename(columns={0: \"me\", 1: \"them\"})\n",
    "\n",
    "# cat_freq_summary['me_over_them'] = cat_freq_summary['me'].div(cat_freq_summary['them'], axis = 0).round(4)\n",
    "# cat_freq_summary['log_me_over_them'] = np.log2(cat_freq_summary['me_over_them'])\n",
    "\n",
    "# # add patterns to df\n",
    "# cat_to_patterns = defaultdict(list)\n",
    "# for patt, cats in dic.items():\n",
    "#     for cat in cats:\n",
    "#         cat_to_patterns[cat].append(patt.pattern)\n",
    "\n",
    "# cat_freq_summary[\"patterns\"] = (\n",
    "#     cat_freq_summary.index.map(lambda c: cat_to_patterns.get(c, []))\n",
    "# )\n",
    "\n",
    "\n",
    "# # print summary\n",
    "# (\n",
    "#     cat_freq_summary\n",
    "#     .sort_values(by=\"log_me_over_them\",key=lambda s: s.abs(), ascending = False)\n",
    "#     .to_csv('results/summary.txt', sep='\\t')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad06d1ba",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Use ConvoKit tool\n",
    "Jonathan P. Chang, Caleb Chiam, Liye Fu, Andrew Wang, Justine Zhang, Cristian Danescu-Niculescu-Mizil. 2020. \"ConvoKit: A Toolkit for the Analysis of Conversations\". Proceedings of SIGDIAL.\n",
    "\n",
    "  1. Download the toolkit: pip3 install convokit\n",
    "  2. Download Spacy's English model: python3 -m spacy download en\n",
    "  3. Download NLTK's 'punkt' model: import nltk; nltk.download('punkt') (in Python interpreter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d90c917",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from convokit import Corpus, Utterance, Speaker, TextParser, Coordination,PolitenessStrategies\n",
    "import nltk; nltk.download('punkt')\n",
    "# spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239d65dd",
   "metadata": {},
   "source": [
    "#### Construct corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8633b55",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "messages.date_time = pd.to_datetime(messages.date_time)\n",
    "df = messages.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fccb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def speaker_id(row):\n",
    "    return \"me\" if row[\"from_me\"] == 1 else \"them\"\n",
    "# make speakers\n",
    "speakers = {\n",
    "    \"me\": Speaker(id=\"me\"),\n",
    "    \"them\": Speaker(id=\"them\")\n",
    "}\n",
    "utterances = []\n",
    "conversation_id = \"sms_conversation_1\"\n",
    "prev_utt_id = None\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    utt_id = f\"utt_{i}\"\n",
    "\n",
    "    utt = Utterance(\n",
    "        id=utt_id,\n",
    "        speaker=speakers[speaker_id(row)],\n",
    "        text=row[\"text\"],\n",
    "        reply_to=prev_utt_id,\n",
    "        conversation_id=conversation_id,\n",
    "        meta={\n",
    "            \"timestamp\": row[\"date_time\"].isoformat(),\n",
    "            \"from_me\": row[\"from_me\"],\n",
    "            \"sender\": row[\"sender\"]\n",
    "            # \"tokens\": row[\"tokens\"]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    utterances.append(utt)\n",
    "    prev_utt_id = utt_id\n",
    "\n",
    "corpus = Corpus(\n",
    "    utterances=utterances\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5c725e",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bf37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = TextParser('en_core_web_sm')\n",
    "corpus = parser.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7483d056",
   "metadata": {},
   "source": [
    "### Analyze Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5150e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.print_summary_stats()\n",
    "\n",
    "[t[\"tok\"] for sent in corpus.get_utterance(\"utt_0\").meta[\"en_core_web_sm\"] for t in sent[\"toks\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c07db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Speakers in corpus:\", list(corpus.iter_speakers()))  \n",
    "print(corpus.speaking_pairs(speaker_ids_only=True)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad677e2f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Speaker Coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speaker coordination\n",
    "coord = Coordination(target_thresh=3, speaker_thresh=5, utterances_thresh=5)  \n",
    "\n",
    "coord.fit(corpus)  \n",
    "\n",
    "coord.transform(corpus)\n",
    "\n",
    "me_coord_scores = corpus.get_speaker(\"me\").meta[\"coord\"]['them']\n",
    "them_coord_scores = corpus.get_speaker(\"them\").meta[\"coord\"]['me']\n",
    "feature_freqs = pd.concat([pd.Series(me_coord_scores).rename(\"me_to_them\"), pd.Series(them_coord_scores).rename(\"them_to_me\")], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a3c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_freqs['diff'] = feature_freqs['me_to_them']-feature_freqs['them_to_me']\n",
    "\n",
    "feature_freqs.sort_values('diff', key = lambda x: abs(x), ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0d697",
   "metadata": {},
   "source": [
    "|Feature|me_to_them|them_to_me|Interpretation|\n",
    "|---|---|---|---|\n",
    "|auxverb|0.31|0.05|You strongly accommodate their auxiliary verbs; they barely adapt to yours|\n",
    "|pronoun|−0.02|0.21|You slightly diverge; they strongly accommodate|\n",
    "|article|0.00|0.00|No coordination either way|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2241561",
   "metadata": {},
   "source": [
    "### Politeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9d548",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Initialize politeness analyzer (requires parsed text)  \n",
    "ps = PolitenessStrategies(parse_attribute_name=\"en_core_web_sm\")  \n",
    "corpus= ps.fit_transform(corpus)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec48d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# Get politeness scores for each speaker's utterances  \n",
    "me_utterances = list(corpus.iter_utterances(lambda x: x.speaker.id == \"me\"))  \n",
    "them_utterances = list(corpus.iter_utterances(lambda x: x.speaker.id == \"them\"))  \n",
    "  \n",
    "# Calculate average politeness strategies per speaker  \n",
    "me_strategies = pd.DataFrame([utt.meta[\"politeness_strategies\"] for utt in me_utterances])\n",
    "them_strategies = pd.DataFrame([utt.meta[\"politeness_strategies\"] for utt in them_utterances])\n",
    "\n",
    "out = pd.concat([me_strategies.sum(0), them_strategies.sum(0)],axis = 1)\n",
    "out.columns = ['me', 'them']\n",
    "out.index = [re.sub(pattern = r'feature_politeness_|==',repl = '', string= x) for x in out.index]\n",
    "\n",
    "# normalize by total utterances spoken\n",
    "utt_counts = Counter(\n",
    "    utt.speaker.id\n",
    "    for utt in corpus.iter_utterances()\n",
    ")\n",
    "\n",
    "out['me_self_normalized'] = out.me/utt_counts['me']\n",
    "out['them_self_normalized'] = out.them/utt_counts['them']\n",
    "\n",
    "out['diff'] = out.me_self_normalized-out.them_self_normalized\n",
    "\n",
    "out.sort_values('diff', key = lambda x: abs(x), ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0198d4",
   "metadata": {},
   "source": [
    "Politeness features\n",
    "\n",
    "| Feature name              | What it measures | Typical interpretation in discourse analysis |\n",
    "|---------------------------|------------------|----------------------------------------------|\n",
    "| Please                    | Presence of the word “please” anywhere in the utterance | Politeness marker; mitigates imposition |\n",
    "| Please_start              | Utterance begins with “please” | High politeness or deference at turn entry |\n",
    "| HASHEDGE                  | Any hedge expression (aggregate indicator) | Linguistic uncertainty, softening, or non-commitment |\n",
    "| Indirect_(btw)            | Indirect discourse marker such as “by the way” | Topic shift or low-imposition insertion |\n",
    "| Hedges                    | Count or presence of hedging terms (e.g., “maybe”, “kind of”) | Reduced certainty; politeness or epistemic caution |\n",
    "| Factuality                | Use of factual/assertive language | Speaker presents information as objective or certain |\n",
    "| Deference                 | Deferential language (e.g., “if you don’t mind”) | Power asymmetry or respect toward interlocutor |\n",
    "| Gratitude                 | Expressions of thanks | Positive social signaling; rapport maintenance |\n",
    "| Apologizing               | Apologies or regret expressions | Face-saving, repair, or politeness strategy |\n",
    "| 1st_person_pl.            | First-person plural pronouns (“we”, “us”) | Inclusivity, shared responsibility, alignment |\n",
    "| 1st_person                | First-person singular pronouns (“I”, “me”) | Self-focus, agency, or ownership of stance |\n",
    "| 1st_person_start          | Utterance begins with a first-person pronoun | Self-initiated stance or framing |\n",
    "| 2nd_person                | Second-person pronouns (“you”) | Addressing, directing, or engaging the interlocutor |\n",
    "| 2nd_person_start          | Utterance begins with a second-person pronoun | Direct engagement; can signal instruction or confrontation |\n",
    "| Indirect_(greeting)       | Indirect greeting (e.g., “hey”, “hope you’re well”) | Social lubrication before substantive content |\n",
    "| Direct_question           | Explicit interrogative form | Information-seeking or directive questioning |\n",
    "| Direct_start              | Utterance begins with a direct request or statement | Low mitigation; task-oriented or assertive style |\n",
    "| HASPOSITIVE               | Presence of positive-affect words | Positive sentiment or encouragement |\n",
    "| HASNEGATIVE               | Presence of negative-affect words | Criticism, frustration, or negative sentiment |\n",
    "| SUBJUNCTIVE               | Subjunctive or hypothetical constructions (“would”, “could”) | Politeness, mitigation, or counterfactual framing |\n",
    "| INDICATIVE                | Indicative (statement-of-fact) constructions | Assertion, certainty, or declarative stance |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8bc383",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x  in dir(corpus) if not bool(re.search(r'^_', x)) ]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/Users/canderson/miniconda3/envs/generic-python/bin/python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
