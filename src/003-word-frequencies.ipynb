{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff78a2-5fbd-4254-b0e2-38b90c6a7c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/Users/canderson/miniconda3/envs/generic-python/bin/python\n",
    "#%%\n",
    "import sqlite3\n",
    "import os\n",
    "from pathlib import Path as pth\n",
    "import subprocess as sp\n",
    "import warnings\n",
    "from typing import List, Tuple, Any, Dict\n",
    "from itertools import chain\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot  as plt\n",
    "import seaborn  as sns\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf4ffd-3dd5-45fe-895d-db6c02aa4dc9",
   "metadata": {},
   "source": [
    " ## Change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a0865d-5b8c-4643-8722-bb2918f5936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(pth(pth.home() / 'dev/sms-analysis'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aee5df-c2e7-4624-8f9a-3256bbeae2e2",
   "metadata": {},
   "source": [
    " ## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5431f7f8-4e0d-47d8-8cd2-ad0d65447673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date_time",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "from_me",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sender",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "4901f99c-e374-4bc5-9176-edcd41f56ef5",
       "rows": [
        [
         "0",
         "2021-03-11 17:25:57",
         "So when is a good time? ",
         "1",
         "13608307613"
        ],
        [
         "1",
         "2021-03-11 17:26:37",
         "Tuesday’s I’m free after 1130",
         "1",
         "13608307613"
        ],
        [
         "2",
         "2021-03-11 23:51:13",
         "this is Christian right? ",
         "0",
         "13608307613"
        ],
        [
         "3",
         "2021-03-11 23:52:07",
         "im done at 3 on tuesday’s so anytime after that works for me!",
         "0",
         "13608307613"
        ],
        [
         "4",
         "2021-03-12 09:17:13",
         "Sounds  like a plan",
         "1",
         "13608307613"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>text</th>\n",
       "      <th>from_me</th>\n",
       "      <th>sender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-11 17:25:57</td>\n",
       "      <td>So when is a good time?</td>\n",
       "      <td>1</td>\n",
       "      <td>13608307613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-11 17:26:37</td>\n",
       "      <td>Tuesday’s I’m free after 1130</td>\n",
       "      <td>1</td>\n",
       "      <td>13608307613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-11 23:51:13</td>\n",
       "      <td>this is Christian right?</td>\n",
       "      <td>0</td>\n",
       "      <td>13608307613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-11 23:52:07</td>\n",
       "      <td>im done at 3 on tuesday’s so anytime after tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>13608307613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-12 09:17:13</td>\n",
       "      <td>Sounds  like a plan</td>\n",
       "      <td>1</td>\n",
       "      <td>13608307613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date_time                                               text  \\\n",
       "0  2021-03-11 17:25:57                           So when is a good time?    \n",
       "1  2021-03-11 17:26:37                      Tuesday’s I’m free after 1130   \n",
       "2  2021-03-11 23:51:13                          this is Christian right?    \n",
       "3  2021-03-11 23:52:07  im done at 3 on tuesday’s so anytime after tha...   \n",
       "4  2021-03-12 09:17:13                                Sounds  like a plan   \n",
       "\n",
       "   from_me       sender  \n",
       "0        1  13608307613  \n",
       "1        1  13608307613  \n",
       "2        0  13608307613  \n",
       "3        0  13608307613  \n",
       "4        1  13608307613  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = pd.read_csv('processed-data/X-messages.csv')\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee78401-04f1-41eb-bf17-d876eb2d4623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize messages by sender\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def tokenize(text):\n",
    "    return [t.text.lower() for t in nlp(text) if t.is_alpha]\n",
    "\n",
    "messages = (\n",
    "    messages\n",
    "        .dropna(subset=[\"text\"])\n",
    "        .assign(tokens=lambda df: df[\"text\"].apply(tokenize))\n",
    ")\n",
    "\n",
    "# # Filter out stop words\n",
    "# stop_words = set(STOP_WORDS)\n",
    "# messages[\"tokens\"] = messages[\"tokens\"].apply(\n",
    "#     lambda toks: [t for t in toks if t not in stop_words]\n",
    "# )\n",
    "\n",
    "\n",
    "# filter for non_empty tokens\n",
    "messages = messages[messages.tokens.apply(len) != 0]\n",
    "\n",
    "\n",
    "my_tokens = (\n",
    "    messages[\"tokens\"][messages.from_me==1]\n",
    "    .explode()\n",
    "    .rename(\"token\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "their_tokens = (\n",
    "    messages[\"tokens\"][messages.from_me==0]\n",
    "    .explode()\n",
    "    .rename(\"token\")\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a46504-037a-402e-ae6a-5be63c859c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "token",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "82976d2d-d968-40f8-bb11-5f0be0fa63fe",
       "rows": [
        [
         "i",
         "2763"
        ],
        [
         "you",
         "2044"
        ],
        [
         "to",
         "1406"
        ],
        [
         "the",
         "858"
        ],
        [
         "and",
         "738"
        ],
        [
         "it",
         "715"
        ],
        [
         "a",
         "710"
        ],
        [
         "so",
         "615"
        ],
        [
         "at",
         "514"
        ],
        [
         "my",
         "507"
        ],
        [
         "that",
         "478"
        ],
        [
         "for",
         "439"
        ],
        [
         "have",
         "432"
        ],
        [
         "in",
         "405"
        ],
        [
         "can",
         "400"
        ],
        [
         "do",
         "390"
        ],
        [
         "we",
         "384"
        ],
        [
         "just",
         "356"
        ],
        [
         "be",
         "355"
        ],
        [
         "is",
         "355"
        ],
        [
         "me",
         "351"
        ],
        [
         "love",
         "350"
        ],
        [
         "if",
         "349"
        ],
        [
         "loved",
         "323"
        ],
        [
         "are",
         "309"
        ],
        [
         "this",
         "292"
        ],
        [
         "want",
         "283"
        ],
        [
         "your",
         "278"
        ],
        [
         "too",
         "276"
        ],
        [
         "but",
         "273"
        ],
        [
         "on",
         "265"
        ],
        [
         "of",
         "260"
        ],
        [
         "was",
         "255"
        ],
        [
         "will",
         "253"
        ],
        [
         "okay",
         "253"
        ],
        [
         "good",
         "228"
        ],
        [
         "with",
         "225"
        ],
        [
         "up",
         "200"
        ],
        [
         "about",
         "196"
        ],
        [
         "yes",
         "193"
        ],
        [
         "would",
         "191"
        ],
        [
         "out",
         "184"
        ],
        [
         "like",
         "173"
        ],
        [
         "get",
         "162"
        ],
        [
         "not",
         "154"
        ],
        [
         "going",
         "153"
        ],
        [
         "did",
         "153"
        ],
        [
         "still",
         "147"
        ],
        [
         "also",
         "146"
        ],
        [
         "go",
         "146"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 3582
       }
      },
      "text/plain": [
       "token\n",
       "i            2763\n",
       "you          2044\n",
       "to           1406\n",
       "the           858\n",
       "and           738\n",
       "             ... \n",
       "codes           1\n",
       "scale           1\n",
       "invisible       1\n",
       "keeses          1\n",
       "exam            1\n",
       "Name: count, Length: 3582, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tokens.value_counts().sort_values( ascending = False)\n",
    "their_tokens.value_counts().sort_values( ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5b4b65-8d92-4a86-8aac-76ad569d9be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patt = re.compile(r'love')\n",
    "their_token_total = their_tokens.shape[0]\n",
    "my_token_total = my_tokens.shape[0]\n",
    "\n",
    "def print_word_count(pattern: str):\n",
    "    print(pattern)\n",
    "    \n",
    "    their_count = (their_tokens.str.contains(pattern, case=False)).sum()\n",
    "    my_count = (my_tokens.str.contains(pattern, case=False)).sum()\n",
    "\n",
    "    my_prop = round(100*my_count/my_token_total, 2)\n",
    "    their_prop = round(100*their_count/their_token_total, 2)\n",
    "\n",
    "    print(\"  Them: \", their_count, f'({their_prop}%)')\n",
    "    print(\"  Me: \", my_count, f'({my_prop}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fed6d0-2ec5-4271-abe6-15ffabf01384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "  Them:  710 (1.62%)\n",
      "  Me:  578 (2.05%)\n",
      "like\n",
      "  Them:  250 (0.57%)\n",
      "  Me:  286 (1.01%)\n",
      "happy\n",
      "  Them:  30 (0.07%)\n",
      "  Me:  21 (0.07%)\n",
      "sad\n",
      "  Them:  20 (0.05%)\n",
      "  Me:  12 (0.04%)\n",
      "miss\n",
      "  Them:  103 (0.24%)\n",
      "  Me:  117 (0.41%)\n",
      "wish\n",
      "  Them:  15 (0.03%)\n",
      "  Me:  27 (0.1%)\n",
      "hat\n",
      "  Them:  653 (1.49%)\n",
      "  Me:  471 (1.67%)\n",
      "dog\n",
      "  Them:  4 (0.01%)\n",
      "  Me:  3 (0.01%)\n",
      "mad\n",
      "  Them:  37 (0.08%)\n",
      "  Me:  27 (0.1%)\n",
      "time\n",
      "  Them:  170 (0.39%)\n",
      "  Me:  77 (0.27%)\n",
      "plan\n",
      "  Them:  77 (0.18%)\n",
      "  Me:  40 (0.14%)\n",
      "you\n",
      "  Them:  2337 (5.33%)\n",
      "  Me:  1637 (5.8%)\n",
      "me\n",
      "  Them:  1244 (2.84%)\n",
      "  Me:  832 (2.95%)\n",
      "how|why|where|when\n",
      "  Them:  362 (0.83%)\n",
      "  Me:  331 (1.17%)\n",
      "sex\n",
      "  Them:  7 (0.02%)\n",
      "  Me:  10 (0.04%)\n"
     ]
    }
   ],
   "source": [
    "patterns = ['love', 'like', 'happy', 'sad', 'miss', 'wish',  \n",
    "            'hat', 'dog', 'mad', 'time', 'plan', 'you', 'me', \n",
    "            'how|why|where|when', 'sex']\n",
    "for patt in patterns:\n",
    "    print_word_count(patt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd14265-bbc3-4205-aa67-e099297502a0",
   "metadata": {},
   "source": [
    " ## use LWIC dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e4aac-c756-47b6-9641-8851ddf0003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dictionary\n",
    "if not pth.exists(pth(\"raw-data/LIWC2007.dic\")):\n",
    "    sp.run([\n",
    "        'curl',\n",
    "        '-L',\n",
    "        'https://raw.githubusercontent.com/Harsh-Panchal-1403/LIWC_PROJECT/master/LIWC2007_English100131.dic',\n",
    "        '-o',\n",
    "        'raw-data/LIWC2007.dic'\n",
    "    ])\n",
    "\n",
    "# #%% [markdown]\n",
    "# # ### Read in dictionary \n",
    "\n",
    "# #%%\n",
    "# def read_dic(path: str) -> Dict[re.Pattern, list[str]]:\n",
    "#     with open(path, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "#     # find lines with field names\n",
    "#     key_marker = [s.strip() == '%' for s in lines]\n",
    "#     key_marker_indx = np.where(key_marker)[0]\n",
    "\n",
    "#     # save keys as dict of key: refnum\n",
    "#     full_keys = lines[key_marker_indx[0] + 1 : key_marker_indx[1] ]\n",
    "#     values = lines[key_marker_indx[1]+1:]\n",
    "\n",
    "#     ref_num_dict = {}\n",
    "#     pat = re.compile(r'^(\\d+)\\t(.+)')\n",
    "#     for s in full_keys:\n",
    "#         m = pat.search(s)\n",
    "#         if m:\n",
    "#             ref_num_dict[int(m.group(1))] = m.group(2).strip()\n",
    "\n",
    "#     # save strings as dict string: refnum\n",
    "#     string_dict = {}\n",
    "#     # val = \"sdlfkj   130294  13294   130459\"\n",
    "#     for val in values:\n",
    "#         refs = [int(x) for x in re.findall(r'\\d+', val)]\n",
    "#         string = val.split('\\t', 1)[0].strip()\n",
    "#         string = re.sub(pattern=r'\\s+',repl= '', string=string)\n",
    "#         string_dict.setdefault(string, []).extend(refs)\n",
    "\n",
    "#     # make one unified dict with key: strings\n",
    "#     full_dict = {}\n",
    "#     for string, ref_nums in string_dict.items():\n",
    "#         cats = []\n",
    "#         for ref_num in ref_nums:\n",
    "#             if ref_num in ref_num_dict:\n",
    "#                 cats.append(ref_num_dict[ref_num])\n",
    "#         string = re.compile(\"^\" + re.escape(string).replace(r\"\\*\", \".*\") + \"$\")\n",
    "#         full_dict[string] = cats\n",
    "\n",
    "#     return full_dict, [re.sub(r'\\d+\\t|\\n', '', x) for x in full_keys]\n",
    "\n",
    "# #%% \n",
    "\n",
    "# # Read dic\n",
    "# dic, categories = read_dic(\"raw-data/LIWC2007.dic\")\n",
    "\n",
    "\n",
    "# #%% [markdown]\n",
    "# # ### Map Texts to Categories\n",
    "\n",
    "# #%%\n",
    "\n",
    "# # return categories for word\n",
    "# def get_categories(tokens) :\n",
    "#     res = []\n",
    "#     for tok in tokens:\n",
    "#         res.append([\n",
    "#             cat\n",
    "#             for patt, cats in dic.items()\n",
    "#             if patt.match(tok)\n",
    "#             for cat in cats\n",
    "#         ])\n",
    "\n",
    "#     # category x message series\n",
    "#     res = pd.Series(res).explode().value_counts()\n",
    "\n",
    "#     # add missing categories\n",
    "#     if not res.shape[0] == len(categories) :\n",
    "#         diff = set(categories).difference(set(res.index))\n",
    "#         add = pd.Series(0,  index = list(diff))\n",
    "#         res = pd.concat([res, add], axis = 0)\n",
    "\n",
    "#     res = res.sort_index()\n",
    "\n",
    "#     return res\n",
    "\n",
    "# #%%\n",
    "# # get_categories( messages.loc[1,'tokens'])\n",
    "\n",
    "# #%% \n",
    "\n",
    "# # Get categories for each message as vector of category counts\n",
    "# category_counts = pd.DataFrame([get_categories(x) for x in messages.tokens])\n",
    "# category_counts.shape\n",
    "# messages.shape\n",
    "\n",
    "# messages.reset_index(drop = True, inplace = True)\n",
    "# category_counts.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# messages.index\n",
    "# messages.columns\n",
    "# category_counts.index\n",
    "# category_counts.columns\n",
    "\n",
    "# x = pd.concat([messages, category_counts], axis = 1)\n",
    "\n",
    "# if x.shape[0] == messages.shape[0]:\n",
    "#     messages = x\n",
    "# else:\n",
    "#     warnings.warn(\n",
    "#         \"Row count mismatch: x does not match messages; assignment skipped.\",\n",
    "#         UserWarning\n",
    "#     )\n",
    "\n",
    "# #%% [markdown]\n",
    "# # ## Compare category counts\n",
    "\n",
    "# #%%\n",
    "# my_cat_counts = messages.loc[messages['from_me']==1,'achieve':].sum(axis = 0)\n",
    "# their_cat_counts = messages.loc[messages['from_me']==0,'achieve':].sum(axis = 0)\n",
    "\n",
    "# # normalize counts\n",
    "# # total_cat_counts = sum(my_cat_counts, their_cat_counts)\n",
    "# my_cat_freq = my_cat_counts/my_cat_counts.sum(0)\n",
    "# their_cat_freq = their_cat_counts/their_cat_counts.sum(0)\n",
    "\n",
    "# cat_freq_summary = pd.concat([my_cat_freq, their_cat_freq], axis = 1).rename(columns={0: \"me\", 1: \"them\"})\n",
    "\n",
    "# cat_freq_summary['me_over_them'] = cat_freq_summary['me'].div(cat_freq_summary['them'], axis = 0).round(4)\n",
    "# cat_freq_summary['log_me_over_them'] = np.log2(cat_freq_summary['me_over_them'])\n",
    "\n",
    "# # add patterns to df\n",
    "# cat_to_patterns = defaultdict(list)\n",
    "# for patt, cats in dic.items():\n",
    "#     for cat in cats:\n",
    "#         cat_to_patterns[cat].append(patt.pattern)\n",
    "\n",
    "# cat_freq_summary[\"patterns\"] = (\n",
    "#     cat_freq_summary.index.map(lambda c: cat_to_patterns.get(c, []))\n",
    "# )\n",
    "\n",
    "\n",
    "# # print summary\n",
    "# (\n",
    "#     cat_freq_summary\n",
    "#     .sort_values(by=\"log_me_over_them\",key=lambda s: s.abs(), ascending = False)\n",
    "#     .to_csv('results/summary.txt', sep='\\t')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b2e3ba-38c9-4eb6-ac9b-7ebae1f5a9b3",
   "metadata": {},
   "source": [
    " ## Use ConvoKit tool\n",
    " Jonathan P. Chang, Caleb Chiam, Liye Fu, Andrew Wang, Justine Zhang, Cristian Danescu-Niculescu-Mizil. 2020. \"ConvoKit: A Toolkit for the Analysis of Conversations\". Proceedings of SIGDIAL.\n",
    "\n",
    "   1. Download the toolkit: pip3 install convokit\n",
    "   2. Download Spacy's English model: python3 -m spacy download en\n",
    "   3. Download NLTK's 'punkt' model: import nltk; nltk.download('punkt') (in Python interpreter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7a1f3b-f0b5-447d-8993-9cb376ae133f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/canderson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from convokit import Corpus, Utterance, Speaker, TextParser, Coordination,PolitenessStrategies\n",
    "import nltk; nltk.download('punkt')\n",
    "# spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd4f13f-ef86-4914-990f-3391f6064c05",
   "metadata": {},
   "source": [
    " #### Construct corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdada55f-4fb5-452d-9664-18e89de78722",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.date_time = pd.to_datetime(messages.date_time)\n",
    "df = messages.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44032113-c8cc-4ddb-b9e1-9e4d63cb178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speaker_id(row):\n",
    "    return \"me\" if row[\"from_me\"] == 1 else \"them\"\n",
    "# make speakers\n",
    "speakers = {\n",
    "    \"me\": Speaker(id=\"me\"),\n",
    "    \"them\": Speaker(id=\"them\")\n",
    "}\n",
    "utterances = []\n",
    "conversation_id = \"sms_conversation_1\"\n",
    "prev_utt_id = None\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    utt_id = f\"utt_{i}\"\n",
    "\n",
    "    utt = Utterance(\n",
    "        id=utt_id,\n",
    "        speaker=speakers[speaker_id(row)],\n",
    "        text=row[\"text\"],\n",
    "        reply_to=prev_utt_id,\n",
    "        conversation_id=conversation_id,\n",
    "        meta={\n",
    "            \"timestamp\": row[\"date_time\"].isoformat(),\n",
    "            \"from_me\": row[\"from_me\"],\n",
    "            \"sender\": row[\"sender\"]\n",
    "            # \"tokens\": row[\"tokens\"]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    utterances.append(utt)\n",
    "    prev_utt_id = utt_id\n",
    "\n",
    "corpus = Corpus(\n",
    "    utterances=utterances\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5624d029-c405-4e0d-9650-5ffcca957760",
   "metadata": {},
   "source": [
    " ### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf04904f-2696-4e5b-8347-56b31a477837",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = TextParser('en_core_web_sm')\n",
    "corpus = parser.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf1d410-5904-4e65-92ca-628af3f89193",
   "metadata": {},
   "source": [
    " ### Analyze Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede58f77-9503-4c27-a582-98280f4ebba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Speakers: 2\n",
      "Number of Utterances: 9262\n",
      "Number of Conversations: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['So', 'when', 'is', 'a', 'good', 'time', '?']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.print_summary_stats()\n",
    "\n",
    "[t[\"tok\"] for sent in corpus.get_utterance(\"utt_0\").meta[\"en_core_web_sm\"] for t in sent[\"toks\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99bf637-aeac-4e9e-90a1-5ac8c9b58311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speakers in corpus: [Speaker({'obj_type': 'speaker', 'vectors': [], 'owner': <convokit.model.corpus.Corpus object at 0x3463d8a10>, 'id': 'me', 'meta': ConvoKitMeta({})}), Speaker({'obj_type': 'speaker', 'vectors': [], 'owner': <convokit.model.corpus.Corpus object at 0x3463d8a10>, 'id': 'them', 'meta': ConvoKitMeta({})})]\n",
      "{('me', 'them'), ('them', 'them'), ('me', 'me'), ('them', 'me')}\n"
     ]
    }
   ],
   "source": [
    "print(\"Speakers in corpus:\", list(corpus.iter_speakers()))  \n",
    "print(corpus.speaking_pairs(speaker_ids_only=True)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4bc384-f778-41f2-8cf6-0c81d740d593",
   "metadata": {},
   "source": [
    " ### Speaker Coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2ea4d-224a-4f04-ac01-8089ad79ab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speaker coordination\n",
    "coord = Coordination(target_thresh=3, speaker_thresh=5, utterances_thresh=5)  \n",
    "\n",
    "coord.fit(corpus)  \n",
    "\n",
    "coord.transform(corpus)\n",
    "\n",
    "me_coord_scores = corpus.get_speaker(\"me\").meta[\"coord\"]['them']\n",
    "them_coord_scores = corpus.get_speaker(\"them\").meta[\"coord\"]['me']\n",
    "feature_freqs = pd.concat([pd.Series(me_coord_scores).rename(\"me_to_them\"), pd.Series(them_coord_scores).rename(\"them_to_me\")], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca3973f-ee25-4a84-8df6-cd94f3a72327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "me_to_them",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "them_to_me",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "diff",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "43bb878b-7db3-4110-8fe0-45edc0ab386a",
       "rows": [
        [
         "adverb",
         "0.06673198204043496",
         "0.09781284711390009",
         "-0.031080865073465136"
        ],
        [
         "ipron",
         "0.1208585749776648",
         "0.15161669628821167",
         "-0.030758121310546865"
        ],
        [
         "quant",
         "0.12311644581650247",
         "0.1529246158507671",
         "-0.02980817003426464"
        ],
        [
         "ppron",
         "0.04047849856802399",
         "0.06893937894431112",
         "-0.028460880376287134"
        ],
        [
         "auxverb",
         "0.0475258637298433",
         "0.06957077030801329",
         "-0.022044906578169987"
        ],
        [
         "preps",
         "0.053115006288567246",
         "0.06677778185822242",
         "-0.013662775569655172"
        ],
        [
         "conj",
         "0.0958595041621732",
         "0.10581207528406905",
         "-0.009952571121895848"
        ],
        [
         "article",
         "0.07797133816616697",
         "0.07883401580824445",
         "-0.0008626776420774807"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>me_to_them</th>\n",
       "      <th>them_to_me</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adverb</th>\n",
       "      <td>0.066732</td>\n",
       "      <td>0.097813</td>\n",
       "      <td>-0.031081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ipron</th>\n",
       "      <td>0.120859</td>\n",
       "      <td>0.151617</td>\n",
       "      <td>-0.030758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quant</th>\n",
       "      <td>0.123116</td>\n",
       "      <td>0.152925</td>\n",
       "      <td>-0.029808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppron</th>\n",
       "      <td>0.040478</td>\n",
       "      <td>0.068939</td>\n",
       "      <td>-0.028461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auxverb</th>\n",
       "      <td>0.047526</td>\n",
       "      <td>0.069571</td>\n",
       "      <td>-0.022045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preps</th>\n",
       "      <td>0.053115</td>\n",
       "      <td>0.066778</td>\n",
       "      <td>-0.013663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conj</th>\n",
       "      <td>0.095860</td>\n",
       "      <td>0.105812</td>\n",
       "      <td>-0.009953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article</th>\n",
       "      <td>0.077971</td>\n",
       "      <td>0.078834</td>\n",
       "      <td>-0.000863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         me_to_them  them_to_me      diff\n",
       "adverb     0.066732    0.097813 -0.031081\n",
       "ipron      0.120859    0.151617 -0.030758\n",
       "quant      0.123116    0.152925 -0.029808\n",
       "ppron      0.040478    0.068939 -0.028461\n",
       "auxverb    0.047526    0.069571 -0.022045\n",
       "preps      0.053115    0.066778 -0.013663\n",
       "conj       0.095860    0.105812 -0.009953\n",
       "article    0.077971    0.078834 -0.000863"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_freqs['diff'] = feature_freqs['me_to_them']-feature_freqs['them_to_me']\n",
    "\n",
    "feature_freqs.sort_values('diff', key = lambda x: abs(x), ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50b9c1-2894-416d-8f69-a94638db071b",
   "metadata": {},
   "source": [
    " |Feature|me_to_them|them_to_me|Interpretation|\n",
    " |---|---|---|---|\n",
    " |auxverb|0.31|0.05|You strongly accommodate their auxiliary verbs; they barely adapt to yours|\n",
    " |pronoun|−0.02|0.21|You slightly diverge; they strongly accommodate|\n",
    " |article|0.00|0.00|No coordination either way|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193874e5-a8ad-4321-831a-cf14a222aa11",
   "metadata": {},
   "source": [
    " ### Politeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a958aa18-9857-40cc-9818-7f259b80c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize politeness analyzer (requires parsed text)  \n",
    "ps = PolitenessStrategies(parse_attribute_name=\"en_core_web_sm\")  \n",
    "corpus= ps.fit_transform(corpus)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98383ee-3543-45e6-af8d-7ff5046b98ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "me",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "them",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "me_self_normalized",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "them_self_normalized",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "diff",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "9819e557-6320-4955-add3-7762679535e1",
       "rows": [
        [
         "1st_person",
         "961",
         "1387",
         "0.22147960359529845",
         "0.28173877716839324",
         "-0.06025917357309479"
        ],
        [
         "2nd_person",
         "1216",
         "1644",
         "0.28024890527771373",
         "0.3339427178549665",
         "-0.05369381257725275"
        ],
        [
         "HASPOSITIVE",
         "1691",
         "2152",
         "0.38972113390182067",
         "0.43713183018484664",
         "-0.047410696283025966"
        ],
        [
         "HASHEDGE",
         "403",
         "678",
         "0.09287854344318967",
         "0.13772090188909203",
         "-0.044842358445902356"
        ],
        [
         "1st_person_start",
         "950",
         "1284",
         "0.21894445724821388",
         "0.26081657525898844",
         "-0.04187211801077456"
        ],
        [
         "1st_person_pl.",
         "217",
         "406",
         "0.05001152339248675",
         "0.08247003859435303",
         "-0.03245851520186629"
        ],
        [
         "Direct_start",
         "112",
         "278",
         "0.02581239917031574",
         "0.05646963233800528",
         "-0.030657233167689542"
        ],
        [
         "Indirect_(greeting)",
         "8",
         "110",
         "0.0018437427978796957",
         "0.022344099126548854",
         "-0.02050035632866916"
        ],
        [
         "HASNEGATIVE",
         "487",
         "645",
         "0.11223784282092648",
         "0.13101767215112736",
         "-0.01877982933020088"
        ],
        [
         "Gratitude",
         "123",
         "217",
         "0.028347545517400322",
         "0.04407881373146456",
         "-0.015731268214064235"
        ],
        [
         "Hedges",
         "165",
         "260",
         "0.038027195206268724",
         "0.05281332520820638",
         "-0.014786130001937653"
        ],
        [
         "Direct_question",
         "201",
         "163",
         "0.046324037796727356",
         "0.03310989234206785",
         "0.013214145454659508"
        ],
        [
         "Deference",
         "114",
         "70",
         "0.026273334869785666",
         "0.014218972171440178",
         "0.012054362698345488"
        ],
        [
         "Factuality",
         "47",
         "98",
         "0.010831988937543212",
         "0.01990656104001625",
         "-0.009074572102473038"
        ],
        [
         "Apologizing",
         "78",
         "131",
         "0.017976492279327034",
         "0.026609790777980907",
         "-0.008633298498653873"
        ],
        [
         "2nd_person_start",
         "130",
         "167",
         "0.029960820465545056",
         "0.033922405037578714",
         "-0.003961584572033657"
        ],
        [
         "INDICATIVE",
         "46",
         "67",
         "0.010601521087808251",
         "0.013609587649807029",
         "-0.0030080665619987776"
        ],
        [
         "SUBJUNCTIVE",
         "16",
         "31",
         "0.0036874855957593914",
         "0.006296973390209222",
         "-0.0026094877944498303"
        ],
        [
         "Please_start",
         "10",
         "4",
         "0.00230467849734962",
         "0.0008125126955108674",
         "0.0014921658018387527"
        ],
        [
         "Please",
         "11",
         "13",
         "0.002535146347084582",
         "0.002640666260410319",
         "-0.00010551991332573719"
        ],
        [
         "Indirect_(btw)",
         "0",
         "0",
         "0.0",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 21
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>me</th>\n",
       "      <th>them</th>\n",
       "      <th>me_self_normalized</th>\n",
       "      <th>them_self_normalized</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1st_person</th>\n",
       "      <td>961</td>\n",
       "      <td>1387</td>\n",
       "      <td>0.221480</td>\n",
       "      <td>0.281739</td>\n",
       "      <td>-0.060259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2nd_person</th>\n",
       "      <td>1216</td>\n",
       "      <td>1644</td>\n",
       "      <td>0.280249</td>\n",
       "      <td>0.333943</td>\n",
       "      <td>-0.053694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HASPOSITIVE</th>\n",
       "      <td>1691</td>\n",
       "      <td>2152</td>\n",
       "      <td>0.389721</td>\n",
       "      <td>0.437132</td>\n",
       "      <td>-0.047411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HASHEDGE</th>\n",
       "      <td>403</td>\n",
       "      <td>678</td>\n",
       "      <td>0.092879</td>\n",
       "      <td>0.137721</td>\n",
       "      <td>-0.044842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st_person_start</th>\n",
       "      <td>950</td>\n",
       "      <td>1284</td>\n",
       "      <td>0.218944</td>\n",
       "      <td>0.260817</td>\n",
       "      <td>-0.041872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st_person_pl.</th>\n",
       "      <td>217</td>\n",
       "      <td>406</td>\n",
       "      <td>0.050012</td>\n",
       "      <td>0.082470</td>\n",
       "      <td>-0.032459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Direct_start</th>\n",
       "      <td>112</td>\n",
       "      <td>278</td>\n",
       "      <td>0.025812</td>\n",
       "      <td>0.056470</td>\n",
       "      <td>-0.030657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indirect_(greeting)</th>\n",
       "      <td>8</td>\n",
       "      <td>110</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>0.022344</td>\n",
       "      <td>-0.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HASNEGATIVE</th>\n",
       "      <td>487</td>\n",
       "      <td>645</td>\n",
       "      <td>0.112238</td>\n",
       "      <td>0.131018</td>\n",
       "      <td>-0.018780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gratitude</th>\n",
       "      <td>123</td>\n",
       "      <td>217</td>\n",
       "      <td>0.028348</td>\n",
       "      <td>0.044079</td>\n",
       "      <td>-0.015731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hedges</th>\n",
       "      <td>165</td>\n",
       "      <td>260</td>\n",
       "      <td>0.038027</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>-0.014786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Direct_question</th>\n",
       "      <td>201</td>\n",
       "      <td>163</td>\n",
       "      <td>0.046324</td>\n",
       "      <td>0.033110</td>\n",
       "      <td>0.013214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deference</th>\n",
       "      <td>114</td>\n",
       "      <td>70</td>\n",
       "      <td>0.026273</td>\n",
       "      <td>0.014219</td>\n",
       "      <td>0.012054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Factuality</th>\n",
       "      <td>47</td>\n",
       "      <td>98</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.009075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apologizing</th>\n",
       "      <td>78</td>\n",
       "      <td>131</td>\n",
       "      <td>0.017976</td>\n",
       "      <td>0.026610</td>\n",
       "      <td>-0.008633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2nd_person_start</th>\n",
       "      <td>130</td>\n",
       "      <td>167</td>\n",
       "      <td>0.029961</td>\n",
       "      <td>0.033922</td>\n",
       "      <td>-0.003962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDICATIVE</th>\n",
       "      <td>46</td>\n",
       "      <td>67</td>\n",
       "      <td>0.010602</td>\n",
       "      <td>0.013610</td>\n",
       "      <td>-0.003008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUBJUNCTIVE</th>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>0.003687</td>\n",
       "      <td>0.006297</td>\n",
       "      <td>-0.002609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Please_start</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.001492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Please</th>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>-0.000106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indirect_(btw)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       me  them  me_self_normalized  them_self_normalized  \\\n",
       "1st_person            961  1387            0.221480              0.281739   \n",
       "2nd_person           1216  1644            0.280249              0.333943   \n",
       "HASPOSITIVE          1691  2152            0.389721              0.437132   \n",
       "HASHEDGE              403   678            0.092879              0.137721   \n",
       "1st_person_start      950  1284            0.218944              0.260817   \n",
       "1st_person_pl.        217   406            0.050012              0.082470   \n",
       "Direct_start          112   278            0.025812              0.056470   \n",
       "Indirect_(greeting)     8   110            0.001844              0.022344   \n",
       "HASNEGATIVE           487   645            0.112238              0.131018   \n",
       "Gratitude             123   217            0.028348              0.044079   \n",
       "Hedges                165   260            0.038027              0.052813   \n",
       "Direct_question       201   163            0.046324              0.033110   \n",
       "Deference             114    70            0.026273              0.014219   \n",
       "Factuality             47    98            0.010832              0.019907   \n",
       "Apologizing            78   131            0.017976              0.026610   \n",
       "2nd_person_start      130   167            0.029961              0.033922   \n",
       "INDICATIVE             46    67            0.010602              0.013610   \n",
       "SUBJUNCTIVE            16    31            0.003687              0.006297   \n",
       "Please_start           10     4            0.002305              0.000813   \n",
       "Please                 11    13            0.002535              0.002641   \n",
       "Indirect_(btw)          0     0            0.000000              0.000000   \n",
       "\n",
       "                         diff  \n",
       "1st_person          -0.060259  \n",
       "2nd_person          -0.053694  \n",
       "HASPOSITIVE         -0.047411  \n",
       "HASHEDGE            -0.044842  \n",
       "1st_person_start    -0.041872  \n",
       "1st_person_pl.      -0.032459  \n",
       "Direct_start        -0.030657  \n",
       "Indirect_(greeting) -0.020500  \n",
       "HASNEGATIVE         -0.018780  \n",
       "Gratitude           -0.015731  \n",
       "Hedges              -0.014786  \n",
       "Direct_question      0.013214  \n",
       "Deference            0.012054  \n",
       "Factuality          -0.009075  \n",
       "Apologizing         -0.008633  \n",
       "2nd_person_start    -0.003962  \n",
       "INDICATIVE          -0.003008  \n",
       "SUBJUNCTIVE         -0.002609  \n",
       "Please_start         0.001492  \n",
       "Please              -0.000106  \n",
       "Indirect_(btw)       0.000000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get politeness scores for each speaker's utterances  \n",
    "me_utterances = list(corpus.iter_utterances(lambda x: x.speaker.id == \"me\"))  \n",
    "them_utterances = list(corpus.iter_utterances(lambda x: x.speaker.id == \"them\"))  \n",
    "  \n",
    "# Calculate average politeness strategies per speaker  \n",
    "me_strategies = pd.DataFrame([utt.meta[\"politeness_strategies\"] for utt in me_utterances])\n",
    "them_strategies = pd.DataFrame([utt.meta[\"politeness_strategies\"] for utt in them_utterances])\n",
    "\n",
    "out = pd.concat([me_strategies.sum(0), them_strategies.sum(0)],axis = 1)\n",
    "out.columns = ['me', 'them']\n",
    "out.index = [re.sub(pattern = r'feature_politeness_|==',repl = '', string= x) for x in out.index]\n",
    "\n",
    "# normalize by total utterances spoken\n",
    "utt_counts = Counter(\n",
    "    utt.speaker.id\n",
    "    for utt in corpus.iter_utterances()\n",
    ")\n",
    "\n",
    "out['me_self_normalized'] = out.me/utt_counts['me']\n",
    "out['them_self_normalized'] = out.them/utt_counts['them']\n",
    "\n",
    "out['diff'] = out.me_self_normalized-out.them_self_normalized\n",
    "\n",
    "out.sort_values('diff', key = lambda x: abs(x), ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71fc906-36f1-491a-9710-a20964c01400",
   "metadata": {},
   "source": [
    " Politeness features\n",
    " | Feature name              | What it measures | Typical interpretation in discourse analysis |\n",
    " |---------------------------|------------------|----------------------------------------------|\n",
    " | Please                    | Presence of the word “please” anywhere in the utterance | Politeness marker; mitigates imposition |\n",
    " | Please_start              | Utterance begins with “please” | High politeness or deference at turn entry |\n",
    " | HASHEDGE                  | Any hedge expression (aggregate indicator) | Linguistic uncertainty, softening, or non-commitment |\n",
    " | Indirect_(btw)            | Indirect discourse marker such as “by the way” | Topic shift or low-imposition insertion |\n",
    " | Hedges                    | Count or presence of hedging terms (e.g., “maybe”, “kind of”) | Reduced certainty; politeness or epistemic caution |\n",
    " | Factuality                | Use of factual/assertive language | Speaker presents information as objective or certain |\n",
    " | Deference                 | Deferential language (e.g., “if you don’t mind”) | Power asymmetry or respect toward interlocutor |\n",
    " | Gratitude                 | Expressions of thanks | Positive social signaling; rapport maintenance |\n",
    " | Apologizing               | Apologies or regret expressions | Face-saving, repair, or politeness strategy |\n",
    " | 1st_person_pl.            | First-person plural pronouns (“we”, “us”) | Inclusivity, shared responsibility, alignment |\n",
    " | 1st_person                | First-person singular pronouns (“I”, “me”) | Self-focus, agency, or ownership of stance |\n",
    " | 1st_person_start          | Utterance begins with a first-person pronoun | Self-initiated stance or framing |\n",
    " | 2nd_person                | Second-person pronouns (“you”) | Addressing, directing, or engaging the interlocutor |\n",
    " | 2nd_person_start          | Utterance begins with a second-person pronoun | Direct engagement; can signal instruction or confrontation |\n",
    " | Indirect_(greeting)       | Indirect greeting (e.g., “hey”, “hope you’re well”) | Social lubrication before substantive content |\n",
    " | Direct_question           | Explicit interrogative form | Information-seeking or directive questioning |\n",
    " | Direct_start              | Utterance begins with a direct request or statement | Low mitigation; task-oriented or assertive style |\n",
    " | HASPOSITIVE               | Presence of positive-affect words | Positive sentiment or encouragement |\n",
    " | HASNEGATIVE               | Presence of negative-affect words | Criticism, frustration, or negative sentiment |\n",
    " | SUBJUNCTIVE               | Subjunctive or hypothetical constructions (“would”, “could”) | Politeness, mitigation, or counterfactual framing |\n",
    " | INDICATIVE                | Indicative (statement-of-fact) constructions | Assertion, certainty, or declarative stance |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
